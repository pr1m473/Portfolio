{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1656789093852,"user":{"displayName":"Ben Manor","userId":"01840878966666339264"},"user_tz":-180},"id":"zeoFz60fceFP","outputId":"054494d5-ecbe-4ac6-ddbc-e12f3a65b7a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Jul  2 19:11:33 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epdrNoX2Pl6R"},"outputs":[],"source":["%%shell\n","\n","# Download TorchVision repo to use some files from\n","# references/detection\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.8.2\n","\n","cp references/detection/utils.py ../\n","cp references/detection/transforms.py ../\n","cp references/detection/coco_eval.py ../\n","cp references/detection/engine.py ../\n","cp references/detection/coco_utils.py ../\n","\n","pip install cython\n","# Install pycocotools, the version by default in Colab\n","# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n","pip install -U torchvision\n","pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","pip install -U albumentations\n","pip install -U opencv-python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmSmrA99JvKB"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsPGCsF6JDkd"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","import albumentations as A\n","import cv2\n","import time\n","from albumentations.pytorch.transforms import ToTensorV2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from  sklearn.model_selection import KFold\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZ6o1qSY__4r"},"outputs":[],"source":["colab_path = 'drive/MyDrive/Primerose 18/Deep project'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtPTkd3pJNOt"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W7pGUzYrHvDN"},"outputs":[],"source":["class CraterDataset(object):\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.imgs = list(sorted(os.listdir(os.path.join(colab_path,root, \"Images\"))))\n","        self.annots = list(sorted(os.listdir(os.path.join(colab_path,root, \"Annotations\"))))\n","        self.classes = ['Background','Crater']\n","\n","    # Converts boundry box formats, this version assumes single class only!\n","    def convert_box_cord(self,bboxs, format_from, format_to, img_shape):\n","        if format_from == 'normxywh':\n","            if format_to == 'xyminmax':\n","                xw = bboxs[:, (1, 3)] * img_shape[1]\n","                yh = bboxs[:, (2, 4)] * img_shape[0]\n","                xmin = xw[:, 0] - xw[:, 1] / 2\n","                xmax = xw[:, 0] + xw[:, 1] / 2\n","                ymin = yh[:, 0] - yh[:, 1] / 2\n","                ymax = yh[:, 0] + yh[:, 1] / 2\n","                coords_converted = np.column_stack((xmin, ymin, xmax, ymax))\n","\n","        return coords_converted\n","\n","    def __getitem__(self, idx):\n","        # load images and boxes\n","        img_path = os.path.join(colab_path,self.root, \"Images\", self.imgs[idx])\n","        annot_path = os.path.join(colab_path,self.root, \"Annotations\", self.annots[idx])\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        # img = cv2.resize(img, (640, 640), cv2.INTER_AREA)\n","        img= img/255.0\n","\n","        # retrieve bbox list and format to required type,\n","        # if annotation file is empty, fill dummy box with label 0\n","        if os.path.getsize(annot_path) != 0:\n","            bboxs = np.loadtxt(annot_path, ndmin=2)\n","            bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)\n","            num_objs = len(bboxs)\n","            bboxs = torch.as_tensor(bboxs, dtype=torch.float32)\n","            # there is only one class\n","            labels = torch.ones((num_objs,), dtype=torch.int64)\n","            # suppose all instances are not crowd\n","            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n","        else:\n","            bboxs = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)\n","            labels = torch.zeros((1,), dtype=torch.int64)\n","            iscrowd = torch.zeros((1,), dtype=torch.int64)\n","\n","        area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])\n","        image_id = torch.tensor([idx])\n","\n","        target = {}\n","        target[\"boxes\"] = bboxs\n","        target[\"labels\"] = labels\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            sample = self.transforms(image=img,\n","                                     bboxes=target['boxes'],\n","                                     labels=labels)\n","        img = sample['image']\n","        target['boxes'] = torch.tensor(sample['bboxes'])\n","        target['labels'] = torch.tensor(sample['labels'])\n","        if target['boxes'].ndim == 1:\n","            target['boxes'] = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)\n","            target['labels'] = torch.zeros((1,), dtype=torch.int64)\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dv9gvnAnH6s6"},"outputs":[],"source":["def get_model_bbox(num_classes):\n","    # load an instance segmentation model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n","    # model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # for param in model.parameters():\n","    #   param.requires_grad = False\n","\n","    # for param in model.roi_heads.box_predictor.parameters():\n","    #   param.requires_grad = True\n","\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0YcKlyybdYw"},"outputs":[],"source":["# model = get_model_bbox(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ub0p7k5QowGe"},"outputs":[],"source":["def get_transform(train):\n","    if train:\n","        return A.Compose([\n","            # A.Flip(p=0.5),\n","            # A.RandomResizedCrop(height=640,width=640,p=0.4),\n","            # # A.Perspective(p=0.4),\n","            # A.Rotate(p=0.5),\n","            # # A.Transpose(p=0.3),\n","            ToTensorV2(p=1.0)],\n","            bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.4, label_fields=['labels']))\n","    else:\n","        return A.Compose([ToTensorV2(p=1.0)],\n","                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdbGkccvo-yF"},"outputs":[],"source":["# Function to visualize bounding boxes in the image\n","def plot_img_bbox(img, target):\n","    # plot the image and bboxes\n","    # Bounding boxes are defined as follows: x-min y-min width height\n","    fig, a = plt.subplots(1, 1)\n","    fig.set_size_inches(5, 5)\n","    a.imshow(img.permute((1,2,0)))\n","    for box in (target['boxes']):\n","        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]\n","        rect = patches.Rectangle((x, y),\n","                                 width, height,\n","                                 edgecolor='b',\n","                                 facecolor='none',\n","                                 clip_on=False)\n","        a.annotate('Crater', (x,y-20), color='blue', weight='bold',\n","                   fontsize=10, ha='left', va='top')\n","\n","        # Draw the bounding box on top of the image\n","        a.add_patch(rect)\n","    plt.show()"]},{"cell_type":"code","source":["dataset = CraterDataset('Craters', get_transform(train=True))\n","# Prints an example of image with annotations\n","for i in random.sample(range(1, 100), 3):\n","    img, target = dataset[i]\n","    plot_img_bbox(img, target)"],"metadata":{"id":"aale9OaaUdED"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edqlOAdCG0sV"},"outputs":[],"source":["def reset_weights(m):\n","  '''\n","    Try resetting model weights to avoid\n","    weight leakage.\n","  '''\n","  for layer in m.children():\n","   if hasattr(layer, 'reset_parameters'):\n","    print(f'Reset trainable parameters of layer = {layer}')\n","    layer.reset_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTHPrjHRrwXp"},"outputs":[],"source":["# train on the GPU or on the CPU, if a GPU is not available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","k_folds = 5\n","num_epochs = 50 \n","\n","\n","# our dataset has two classes only - background and crater\n","num_classes = 2\n","# use our dataset and defined transformations\n","dataset = CraterDataset('Craters', get_transform(train=True))\n","dataset_test = CraterDataset('Craters', get_transform(train=False))\n","\n","# Define the K-fold Cross Validator\n","kfold = KFold(n_splits=k_folds, shuffle=True)\n","\n","# Start print\n","print('--------------------------------')\n","\n","# K-fold Cross Validation model evaluation\n","for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","\n","    dataset_subset = torch.utils.data.Subset(dataset, list(train_ids))\n","    dataset_val_subset = torch.utils.data.Subset(dataset_val, list(val_ids))\n","\n","    # define training and validation data loaders\n","    data_loader = torch.utils.data.DataLoader(\n","            dataset_subset, batch_size=8, shuffle=True, num_workers=2,\n","        collate_fn=utils.collate_fn)\n","\n","    data_loader_val = torch.utils.data.DataLoader(\n","        dataset_val_subset, batch_size=1, shuffle=False, num_workers=2,\n","        collate_fn=utils.collate_fn)\n","\n","    # get the model using our helper function\n","    model = get_model_bbox(num_classes)\n","    \n","    #model.apply(reset_weights) # Check if beneficial\n","    \n","    # move model to the right device\n","    model.to(device)\n","\n","    # construct an optimizer\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.005,  # Check if beneficial\n","                                momentum=0.9, weight_decay=0)\n","\n","    # and a learning rate scheduler\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                    step_size=10,\n","                                                    gamma=0.1)\n","    \n","    # let's train!\n","    for epoch in range(num_epochs):\n","\n","\n","        # train for one epoch, printing every 50 iterations\n","        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n","        # update the learning rate\n","        lr_scheduler.step()\n","        # evaluate on the test dataset\n","    evaluate(model, data_loader_val, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kBttkYMGuJ02"},"outputs":[],"source":["num_epochs = 200 \n","\n","# our dataset has two classes only - background and crater\n","num_classes = 2\n","# use our dataset and defined transformations\n","dataset = CraterDataset('Craters', get_transform(train=True))\n","dataset_test = CraterDataset('Craters/test', get_transform(train=False))\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","        dataset, batch_size=8, shuffle=True, num_workers=2,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=2,\n","    collate_fn=utils.collate_fn)\n","\n","# get the model using our helper function\n","model = get_model_bbox(num_classes)\n","\n","'''\n","Use this to reset all trainable weights\n","model.apply(reset_weights)\n","'''\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,  # Feel free to play with values\n","                            momentum=0.9, weight_decay=0)\n","\n","# Defining learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                step_size=20,\n","                                                gamma=0.2)\n","\n","\n","result_mAP = []\n","best_epoch = None\n","\n","# Let's train!\n","for epoch in range(num_epochs):\n","\n","    \n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    results =  evaluate(model, data_loader_test, device=device)\n","    # saves results of mAP @ IoU = 0.5\n","    result_mAP.append(results.coco_eval['bbox'].stats[1])\n","    #save the best model so far\n","    if result_mAP[-1] == max(result_mAP):\n","        best_save_path = os.path.join(f'Crater_bestmodel_noaug_sgd(wd=0)_8batch-epoch{epoch}.pth')\n","        torch.save(model.state_dict(), best_save_path)\n","        best_epoch = int(epoch)\n","        print(f'\\n\\nmodel from epoch number {epoch} saved!\\n result is {max(result_mAP)}\\n\\n')\n","    \n","# Saving the last model\n","save_path = os.path.join(f'Crater_noaug_sgd_2batch-lastepoch{num_epochs-1}.pth')\n","torch.save(model.state_dict(), save_path)\n","print(f'model from last epoch(no.{num_epochs-1}) saved')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-Kiy8xopvCjj"},"outputs":[],"source":["dataset_test = CraterDataset('Craters/test', get_transform(train=False))\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=2,\n","    collate_fn=utils.collate_fn)\n","\n","model = get_model_bbox(num_classes)\n","\n","# load model to evaluate\n","model.load_state_dict(torch.load(os.path.join(f'Crater_bestmodel_noaug_sgd(wd=0)_8batch-epoch{best_epoch}.pth'),map_location=device))\n","model.to(device)\n","\n","  \n","evaluate(model, data_loader_test, device=device)"]},{"cell_type":"code","source":["# Define colors for bounding boxes\n","color_inference = np.array([0.0,0.0,255.0])\n","color_label = np.array([255.0,0.0,0.0])\n","\n","# Score value thershold for displaying predictions\n","detection_threshold = 0.7\n","# to count the total number of images iterated through\n","frame_count = 0\n","# to keep adding the FPS for each image\n","total_fps = 0\n","\n","!mkdir ./results"],"metadata":{"id":"z7rCVbr6X3FP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i,data in enumerate(data_loader_test):\n","    # get the image file name for predictions file name\n","    image_name = 'image no:' + str(int(data[1][0]['image_id']))\n","    model_image = data[0][0]\n","    cv2_image = np.transpose(model_image.numpy()*255,(1, 2, 0)).astype(np.float32)\n","    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_RGB2BGR).astype(np.float32)\n","\n","    # add batch dimension\n","    model_image = torch.unsqueeze(model_image, 0)\n","    start_time = time.time()\n","    with torch.no_grad():\n","        outputs = model(model_image.to(device))\n","    end_time = time.time()\n","    # get the current fps\n","    fps = 1 / (end_time - start_time)\n","    # add `fps` to `total_fps`\n","    total_fps += fps\n","    # increment frame count\n","    frame_count += 1\n","    # load all detection to CPU for further operations\n","    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n","    # carry further only if there's detected boxes\n","    if len(outputs[0]['boxes']) != 0:\n","        boxes = outputs[0]['boxes'].data.numpy()\n","        scores = outputs[0]['scores'].data.numpy()\n","        # filter out boxes according to `detection_threshold`\n","        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n","        scores = np.round(scores[scores >= detection_threshold],2)\n","        draw_boxes = boxes.copy()\n","\n","\n","        # draw the bounding boxes and write the class name on top of it\n","        for j,box in enumerate(draw_boxes):\n","            cv2.rectangle(cv2_image,\n","                          (int(box[0]), int(box[1])),\n","                          (int(box[2]), int(box[3])),\n","                          color_inference, 2)\n","            cv2.putText(img=cv2_image, text=\"Crater\",\n","                        org=(int(box[0]), int(box[1] - 5)),\n","                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,\n","                        thickness=1, lineType=cv2.LINE_AA)\n","            cv2.putText(img=cv2_image, text=str(scores[j]),\n","                        org=(int(box[0]), int(box[1] + 8)),\n","                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,\n","                        thickness=1, lineType=cv2.LINE_AA)\n","            \n","        # add boxes for labels\n","        for box in data[1][0]['boxes']:\n","            cv2.rectangle(cv2_image,\n","                          (int(box[0]), int(box[1])),\n","                          (int(box[2]), int(box[3])),\n","                          color_label, 2)\n","            cv2.putText(img=cv2_image, text=\"Label\",\n","                        org=(int(box[0]), int(box[1] - 5)),\n","                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_label,\n","                        thickness=1, lineType=cv2.LINE_AA)           \n","            \n","\n","        # set size\n","        plt.figure(figsize=(10,10))\n","        plt.axis(\"off\")\n","\n","        # convert color from CV2 BGR back to RGB\n","        plt_image = cv2.cvtColor(cv2_image/255.0, cv2.COLOR_BGR2RGB)\n","        plt.imshow(plt_image)\n","        plt.show()\n","        cv2.imwrite(f\"./results/{image_name}.jpg\", cv2_image)\n","    print(f\"Image {i + 1} done...\")\n","    print('-' * 50)\n","print('TEST PREDICTIONS COMPLETE')\n","\n","avg_fps = total_fps / frame_count\n","print(f\"Average FPS: {avg_fps:.3f}\")\n"],"metadata":{"id":"EOohTi6dYBGF"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Crater Final train.ipynb","provenance":[{"file_id":"1o4rdsSFx-HGESa_N0QFHSh4tCjJ65rlI","timestamp":1656767006325}],"mount_file_id":"1lIEu08aCWGa2BdxrE7iDHqm5xT4QbaRw","authorship_tag":"ABX9TyNRUDdTKDuYw07JU1dd8Jj3"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}